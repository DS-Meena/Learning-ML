{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df4789fb",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Open-source development framework for LLM applications.\n",
    "Python and JavaScript (TypeScript) packages\n",
    "\n",
    "Focused on composition and modularity.\n",
    "\n",
    "Key value adds:\n",
    "1. Modular components\n",
    "2. Use cases: Common ways to combine components\n",
    "\n",
    "### Models\n",
    "- LLMs: 20+ integrations\n",
    "- Chat Models\n",
    "- Text Embedding Models: 10+ integrations\n",
    "\n",
    "### Prompts\n",
    "- Prompt Templates\n",
    "- Output Parsers: 5+ implementations\n",
    "    - Retry/fixing logic\n",
    "- Example Selectors: 5+ implementations\n",
    "\n",
    "### Indexes\n",
    "- Document Loaders: 50+ implementations\n",
    "- Text Splitters: 10+ implementations\n",
    "- Vector stores: 10+ integrations\n",
    "- Retrievers: 5+ integrations/implementations\n",
    "\n",
    "### Chains\n",
    "- Prompt + LLM + Output parsing\n",
    "- Can be used as building blocks for longer chains\n",
    "- More application specific chains: 20+ types\n",
    "\n",
    "### Agents\n",
    "- Agent Types: 5+ types\n",
    "    - Algorithms for getting LLMs to use tools\n",
    "- Agent Toolkits: 10+ implementations\n",
    "    - Agents armed with specific tools for a specific application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee8eeef",
   "metadata": {},
   "source": [
    "# Models, Prompts and Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b1ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = 'sk-proj-L3Z0WfOdSg4A'\n",
    "\n",
    "from openai import OpenAI \n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY'],   # Default can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cbfbfb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6583111",
   "metadata": {},
   "source": [
    "## Chat API: Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4f29cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\n",
    "        \"role\": \"user\", \n",
    "        \"content\": prompt\n",
    "    }]\n",
    "    response = client.chat.completions.create(\n",
    "        model = model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "035cdf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")\n",
    "\n",
    "# '1+1 equals 2.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e2bfe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"I am really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn\\'t cover the cost of cleaning up my kitchen. I could really use your help right now, friend.\"\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "style = \"\"\"American English in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Translate the text that is delimited by triple backticks into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "# response = get_completion(prompt)\n",
    "# response\n",
    "\n",
    "\"\"\"\n",
    "\"I am really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, friend.\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd93cb",
   "metadata": {},
   "source": [
    "## Chat API : LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489a285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (1.1.0)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: langchain-core in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (1.1.0)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langchain) (1.0.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langchain) (2.12.4)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
      "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain-community)\n",
      "  Downloading sqlalchemy-2.0.44-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langchain-community) (6.0.3)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langchain-community) (9.1.2)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langchain-community) (0.4.49)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langchain-core) (24.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langchain-core) (4.15.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
      "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.10)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2024.8.30)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community)\n",
      "  Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: anyio in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/dsm/training/Learning-ML/.venv/lib/python3.12/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading sqlalchemy-2.0.44-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (607 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.6/607.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: requests, propcache, mypy-extensions, multidict, marshmallow, httpx-sse, greenlet, frozenlist, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langchain-text-splitters, langchain-classic, langchain-community\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.32.3\n",
      "    Uninstalling requests-2.32.3:\n",
      "      Successfully uninstalled requests-2.32.3\n",
      "Successfully installed SQLAlchemy-2.0.44 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 dataclasses-json-0.6.7 frozenlist-1.8.0 greenlet-3.2.4 httpx-sse-0.4.3 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-text-splitters-1.0.0 marshmallow-3.26.1 multidict-6.7.0 mypy-extensions-1.1.0 propcache-0.4.1 pydantic-settings-2.12.0 requests-2.32.5 typing-inspect-0.9.0 yarl-1.22.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade langchain\n",
    "# !pip install -U langchain-openai\n",
    "# !pip install --upgrade langchain langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed1cdd",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3186512e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(profile={'max_input_tokens': 16385, 'max_output_tokens': 4096, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': False, 'structured_output': False, 'image_url_inputs': False, 'pdf_inputs': False, 'pdf_tool_message': False, 'image_tool_message': False, 'tool_choice': True}, client=<openai.resources.chat.completions.completions.Completions object at 0x7445c3dd1490>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7445c3dd3470>, root_client=<openai.OpenAI object at 0x7445d4d76db0>, root_async_client=<openai.AsyncOpenAI object at 0x7445c3dd3680>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain_openai  import ChatOpenAI\n",
    "\n",
    "llm_model = \"gpt-3.5-turbo\"\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3402f3ff",
   "metadata": {},
   "source": [
    "## Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f748f997",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text that is delimited by triple backticks into a style that is {style}.\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "473bbbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['style', 'text'] input_types={} partial_variables={} template='Translate the text that is delimited by triple backticks into a style that is {style}.\\ntext: ```{text}```\\n'\n",
      "['style', 'text']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "print(prompt_template.messages[0].prompt)\n",
    "print(prompt_template.messages[0].prompt.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9bdb8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n",
      "content=\"Translate the text that is delimited by triple backticks into a style that is American English in a calm and respectful tone.\\ntext: ```Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse, the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!```\\n\" additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "customer_style = \"\"\"American English in a calm and respectful tone\"\"\"\n",
    "customer_email = \"\"\"Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\"\"\"\n",
    "\n",
    "customer_messages = prompt_template.format_messages(style=customer_style, text=customer_email)\n",
    "\n",
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))\n",
    "print(customer_messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df82adb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, friend.\n"
     ]
    }
   ],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat.invoke(customer_messages)\n",
    "\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72787b76",
   "metadata": {},
   "source": [
    "## Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24478627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gift': False, 'delivery_days': 5, 'price_value': 'pretty affordable!'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"gift\": False,\n",
    "  \"delivery_days\": 5,\n",
    "  \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cea13e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52e880e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['text'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], input_types={}, partial_variables={}, template='For the following text, extract the following information:\\n\\ngift: Was the item purchased as a gift for someone else? Answer True if yes, False if not or unknown.\\n\\ndelivery_days: How many days did it take for the product to arrive? If this information is not found, output -1.\\n\\nprice_value: Extract any sentences about the value or price, and output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\ngift\\ndelivery_days\\nprice_value\\n\\ntext: {text}\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4507734c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"gift\": true,\n",
      "    \"delivery_days\": 2,\n",
      "    \"price_value\": [\"It's slightly more expensive than the other leaf blowers out there\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d755fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf2f27c",
   "metadata": {},
   "source": [
    "### Parse the LLM output string into a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e140df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic  import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class GiftReview(BaseModel):\n",
    "    gift: bool = Field(\n",
    "        description=(\n",
    "            \"Was the item purchased as a gift for someone else? \"\n",
    "            \"True if yes, False if not or unknown.\"\n",
    "        )\n",
    "    )\n",
    "    delivery_days: int = Field (\n",
    "        description=(\n",
    "            \"How many days did it take for the product to arrive? \"\n",
    "            \"If this information is not found, use -1.\"\n",
    "        )\n",
    "    )\n",
    "    price_value: List[str] = Field (\n",
    "        description=(\n",
    "            \"Sentences about the value or price of the product, \"\n",
    "            \"as a list of strings.\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daad715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gift': True, 'delivery_days': 2, 'price_value': ['slightly more expensive than other leaf blowers', 'worth it for the extra features']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373713/1574473322.py:4: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  result_dict = result_obj.dict()\n"
     ]
    }
   ],
   "source": [
    "structured_llm = chat.with_structured_output(GiftReview)\n",
    "\n",
    "result_obj = structured_llm.invoke(customer_review)\n",
    "result_dict = result_obj.dict()\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40039fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gift': True, 'delivery_days': 2, 'price_value': ['slightly more expensive than other leaf blowers', 'worth it for the extra features']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d321e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict.get('delivery_days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19c480d",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "Outline\n",
    "- ConversationBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryMemory\n",
    "\n",
    "## Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e85978",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai  import ChatOpenAI\n",
    "from langchain_classic.chains import ConversationChain\n",
    "from langchain_classic.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68aeceb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373713/2735856642.py:2: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n",
      "/tmp/ipykernel_373713/2735856642.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use `langchain_core.runnables.history.RunnableWithMessageHistory` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9cb08a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is DSMeena\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello DSMeena! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is DSMeena\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97bfd5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is DSMeena\n",
      "AI: Hello DSMeena! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7368376a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is DSMeena\n",
      "AI: Hello DSMeena! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is DSMeena.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c37a8c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is DSMeena\n",
      "AI: Hello DSMeena! It's nice to meet you. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: 1+1 equals 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is DSMeena.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "228cb28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is DSMeena\\nAI: Hello DSMeena! It's nice to meet you. How can I assist you today?\\nHuman: What is 1+1?\\nAI: 1+1 equals 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is DSMeena.\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da0082a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc5899",
   "metadata": {},
   "source": [
    "## Conversation Buffer Window Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "224f6588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2b7d092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2f3a5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "26cb0f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10d27faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello DSMeena! It's nice to meet you. How can I assist you today?\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is DSMeena\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30e728ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3aab0618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I do not have access to personal information such as your name. Is there anything else you would like to ask?\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaafd1e",
   "metadata": {},
   "source": [
    "## Conversation Token Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4d5f860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.memory import ConversationTokenBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e3c161f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=25)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"},\n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bd309d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0b057c",
   "metadata": {},
   "source": [
    "## Conversation Summary Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "280a0660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8739a039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_373713/850630356.py:10: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"
     ]
    }
   ],
   "source": [
    "# a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "173029de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human and AI exchange greetings and discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI.'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2a5cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "34958c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI exchange greetings and discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'For the meeting with the product team, a good demo to show would be the latest prototype of the LangChain project, highlighting its key features and functionalities. This could include a live demonstration of how the language processing algorithms work, the accuracy of the translation capabilities, and any new updates or improvements that have been made since the last meeting. Additionally, showcasing some real-world examples of how the LangChain project can be used in different industries or applications could help to illustrate its potential value and impact.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7f4f2fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: The human and AI discuss the schedule for the day, including a meeting with the product team, work on the LangChain project, and a lunch meeting with a customer interested in AI. They also talk about what demo would be good to show, with the AI suggesting showcasing the latest prototype of the LangChain project and highlighting its key features and functionalities, as well as providing real-world examples of its potential value and impact.'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0618ce32",
   "metadata": {},
   "source": [
    "# Chains in LangChain\n",
    "\n",
    "- LLMChain\n",
    "- Sequential Chains\n",
    "    - SimpleSequentialChain\n",
    "    - SequentialChain\n",
    "- Router Chain\n",
    "\n",
    "## LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f2a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.prompts import ChatPromptTemplate\n",
    "from langchain_classic.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.0, model=\"gpt-3.5-turbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5389129",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe a company that makes {product}\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53bf24ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Royal Comfort Linens\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size sheet set\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9226e03c",
   "metadata": {},
   "source": [
    "## Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53467f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fda7c0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is teh best name to describe a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f17dc360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following company: {company_name}\"\n",
    ")\n",
    "\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83574af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41dc3329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"Royal Comfort Linens\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\"Royal Comfort Linens offers luxurious and high-quality bedding and linens to elevate your sleep experience and home decor.\"\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"Royal Comfort Linens offers luxurious and high-quality bedding and linens to elevate your sleep experience and home decor.\"'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a66aa2d",
   "metadata": {},
   "source": [
    "## Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7ee592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "199629ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "\n",
    "# chain 1: input = Review and output = English Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key = \"English_Review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95830643",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "\n",
    "# chain 2: input = English_Review and output = summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key = \"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63d2bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review: \\n\\n{Review}\"\n",
    ")\n",
    "\n",
    "# chain 3: input = Review and output = Language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75ae936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "\n",
    "# Chain 4: input = summary, language and output = follow up message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\"followup_message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d540f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_chain: input = Review\n",
    "# and output = English_Review, summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\", \"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "675549e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       " 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's strange. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\",\n",
       " 'summary': 'The reviewer is unimpressed with the taste of the product, finding it to be subpar compared to the ones purchased in stores and wondering if it is a result of them being old or counterfeit.',\n",
       " 'followup_message': \"Je suis désolé que vous n'ayez pas apprécié le goût du produit. Il est possible qu'il soit effectivement vieux ou contrefait. Il serait peut-être utile de contacter le fabricant pour obtenir des éclaircissements et éventuellement un remboursement. Nous espérons que vous trouverez un produit qui répondra à vos attentes à l'avenir.\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\"\n",
    "\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c483a",
   "metadata": {},
   "source": [
    "## Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c60dc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07577838",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4cfdd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.router import MultiPromptChain\n",
    "from langchain_classic.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8eefb32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "eae38ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physics: Good for answering questions about physics\n",
      "math: Good for answering math questions\n",
      "History: Good for answering history questions\n",
      "computer science: Good for answering computer science questions\n"
     ]
    }
   ],
   "source": [
    "print(destinations_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a6d6824",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "696e0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in {destinations}\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: The value of “destination” MUST match one of the candidate prompts listed below.\\\n",
    "If “destination” does not fit any of the specified prompts, set it to “DEFAULT.”\n",
    "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f212192",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "56fb19be",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain,\n",
    "                        destination_chains=destination_chains,\n",
    "                        default_chain=default_chain, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b219be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an idealized object that absorbs all radiation incident upon it and emits radiation at all frequencies and temperatures. The spectral distribution of black body radiation is described by Planck's law, which shows that the intensity of radiation increases with frequency and temperature. This phenomenon is important in understanding the behavior of objects at high temperatures, such as stars, and in the development of quantum mechanics.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89232a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'What is 2 + 2?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The answer to 2 + 2 is 4.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is 2 + 2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a548fcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "None: {'input': 'Why does every cell in our body contain DNA?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Every cell in our body contains DNA because DNA carries the genetic information that determines the characteristics and functions of that cell. It serves as the instructions for building and maintaining the structure and activities of the cell. Additionally, DNA is required for cell division, growth, and repair. Therefore, DNA is essential for the proper functioning and survival of all cells in the body.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3507e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
