{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/dsm/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/dsm/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')  # required for tokenization\n",
    "nltk.download('stopwords')   # required to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'you', 'love', 'NLP', '?', 'It', \"'s\", 'amazing', '!']\n",
      "[\"Don't you love NLP?\", \"It's amazing!\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = \"Don't you love NLP? It's amazing!\"\n",
    "\n",
    "words = nltk.word_tokenize(sentence)\n",
    "sentences = nltk.sent_tokenize(sentence)\n",
    "\n",
    "print(words)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'natural', 'language', 'processing', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = \"I love Natural Language Processing!\"\n",
    "tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(filtered)  \n",
    "# ['love', 'natural', 'language', 'processing', '!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context free Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         S                   \n",
      "  _______|___                 \n",
      " |           VP              \n",
      " |    _______|____            \n",
      " |   |            NP         \n",
      " |   |    ________|____       \n",
      " |   |   |             NP    \n",
      " |   |   |         ____|___   \n",
      " NP  |   |        AP       NP\n",
      " |   |   |        |        |  \n",
      " N   V   D        A        N \n",
      " |   |   |        |        |  \n",
      "she saw  a      small     dog\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Define some formal rules (formal grammar)\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "\n",
    "    AP -> A | A AP\n",
    "    NP -> N | D NP | AP NP | N PP\n",
    "    PP -> P NP\n",
    "    VP -> V | V NP | V NP PP\n",
    "\n",
    "    A -> \"big\" | \"blue\" | \"small\" | \"dry\" | \"wide\"\n",
    "    D -> \"the\" | \"a\" | \"an\"\n",
    "    N -> \"she\" | \"city\" | \"car\" | \"street\" | \"dog\" | \"binoculars\"\n",
    "    P -> \"on\" | \"over\" | \"before\" | \"below\" | \"with\"\n",
    "    V -> \"saw\" | \"walked\"\n",
    "\"\"\")\n",
    "\n",
    "parser = nltk.ChartParser(grammar)\n",
    "\n",
    "# sentence = input(\"Sentence: \").split()\n",
    "sentence = \"she saw a small dog\".split()\n",
    "try:\n",
    "    for tree in parser.parse(sentence):\n",
    "        tree.pretty_print()\n",
    "        tree.draw()\n",
    "        break  # print only a single tree\n",
    "except ValueError:\n",
    "    print(\"No parse tree possible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'cat' encoded: [1, 0, 0]\n",
      "'dog' encoded: [0, 1, 0]\n",
      "'bird' encoded: [0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "# Creating one-hot encodings for a small vocabulary\n",
    "vocabulary = [\"cat\", \"dog\", \"bird\"]\n",
    "\n",
    "def one_hot_encode(word, vocab):\n",
    "    vector = [0] * len(vocab)\n",
    "    if word in vocab:\n",
    "        vector[vocab.index(word)] = 1\n",
    "    return vector\n",
    "\n",
    "# Examples\n",
    "print(f\"'cat' encoded: {one_hot_encode('cat', vocabulary)}\")  # [1, 0, 0]\n",
    "print(f\"'dog' encoded: {one_hot_encode('dog', vocabulary)}\")  # [0, 1, 0]\n",
    "print(f\"'bird' encoded: {one_hot_encode('bird', vocabulary)}\")  # [0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
